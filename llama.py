# -*- coding: utf-8 -*-
"""main2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vXo88nesBFwC766tvtxlPmP3fv-_lEUN
"""

!pip install --quiet peft openai torchviz accelerate
!pip install --quiet --upgrade bitsandbytes

!unzip -qq ./ScienceQA.zip

"""IMPORT PACKAGES"""

import os
import json
import pickle
from tqdm import tqdm
from pprint import pprint

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from torchviz import make_dot

from peft import PromptTuningConfig, PromptTuningInit, TaskType, get_peft_model

from huggingface_hub import login

from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup, BitsAndBytesConfig

from ScienceQA.models.base_prompt import *
from ScienceQA.models.run_gpt3 import *

"""CONFIGURATION"""

LEARNING_RATE = 1e-4
BATCH_SIZE = 1
NUM_EPOCHS = 50
SEQ_LENGTH = 512
SOFT_PROMPT_LENGTH = 64
MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'

tqdm.pandas()

"""IMPORT DATASET"""

class ScienceQADatasetWrapper():
    class Dict2Class(object):
        def __init__(self, my_dict):
            for key in my_dict:
                setattr(self, key, my_dict[key])

    class ScienceQADataset(Dataset):
        def __init__(self, df, shot_qids, args, shot_df, tokenizer):
            super().__init__()
            self.prompts = []

            shot_added_df = pd.concat((df, shot_df))
            shot_added_dict = shot_added_df.transpose().to_dict()

            for qid in tqdm([str(x) for x in df.index if x not in shot_qids]):
                prompt = build_prompt(shot_added_dict, shot_qids, qid, args)

                self.prompts.append(prompt)
            self.answers = df.progress_apply(lambda x : x['choices'][x['answer']], axis = 1).to_list()
            self.distractors = df.progress_apply(lambda x : ';'.join([choice for choice in x['choices'] if choice is not x['choices'][x['answer']]]), axis = 1).to_list()

        def __len__(self):
            return len(self.prompts)

        def __getitem__(self, index):
            return self.prompts[index], self.answers[index], self.distractors[index]

    def __init__(self):
        # Configure Huggingface
        with open('./key.json', 'r') as f:
            data = json.load(f)
        access_token = data['HF_API_KEY']
        login(token = access_token, add_to_git_credential = True, new_session = True)

        # Build ScienceQA Argument
        args_dict = {
            'data_root': './ScienceQA/data/scienceqa/',
            'output_root': './ScienceQA/results/',
            'caption_file': './ScienceQA/data/captions.json',
            'model': 'Llama-3',
            'options': ['A', 'B', 'C', 'D', 'E'],
            'label': 'PLAIN',
            'test_split': 'val',
            'test_number': 3,
            'use_caption': True,
            'save_every': 10,
            'debug': True,
            'prompt_format': 'CQM-A',
            'shot_number': 3,
            'shot_qids': None,
            'seed': 42,
        }
        args = self.Dict2Class(args_dict)

        # Build Dataset
        problems, qids, shot_qids = load_data(args)
        result_file = get_result_file(args)
        df = pd.DataFrame(problems).transpose()

        # Build DataFrames
        train_df = df[df['split'] == 'train']
        valid_df = df[df['split'] == 'val']
        test_df = df[df['split'] == 'test']

        # Build Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        self.tokenizer.pad_token = self.tokenizer.eos_token

        # Build Dataset
        train_dataset = self.ScienceQADataset(train_df, shot_qids, args, df.loc[shot_qids], self.tokenizer)
        valid_dataset = self.ScienceQADataset(valid_df, shot_qids, args, df.loc[shot_qids], self.tokenizer)
        test_dataset = self.ScienceQADataset(test_df, shot_qids, args, df.loc[shot_qids], self.tokenizer)

        # Build Dataloader
        self.train_dataloader = DataLoader(train_dataset, shuffle = False, batch_size = BATCH_SIZE)
        self.valid_dataloader = DataLoader(valid_dataset, shuffle = False, batch_size = BATCH_SIZE)
        self.test_dataloader = DataLoader(test_dataset, shuffle = False, batch_size = BATCH_SIZE)

    def get_loaders(self):
        return self.train_dataloader, self.valid_dataloader, self.test_dataloader

    def get_tokenizer(self):
        return self.tokenizer

wrapper = ScienceQADatasetWrapper()

train_dataloader, valid_dataloader, test_dataloader = wrapper.get_loaders()

tokenizer = wrapper.get_tokenizer()

"""IMPORT MODEL"""

class Llama3Trainer():
    class FirstHalfLlamaModel(nn.Module):
        def __init__(self, model):
            super().__init__()

            self.embedding_layer = model.get_input_embeddings()
            self.first_half_layers = model.model.layers[:16]

        def forward(self, input_ids):
            output = self.embedding_layer(input_ids)
            for layer in self.first_half_layers:
                output = layer(output)
            return output

    class SecondHalfLlamaModel(nn.Module):
        def __init__(self, model):
            super().__init__()

            self.second_half_layers = model.model.layers[:16]
            self.norm = model.model.norm
            self.lm_head = model.lm_head

        def forward(self, output):
            for layer in self.second_half_layers:
                output = layer(output)

            output = self.norm(output)
            output = self.lm_head(output)
            return output

    def __init__(self, use_peft, use_division, use_quantization, device, train_dataloader, valid_dataloader, test_dataloader, tokenizer):
        # Build Tokenzier & Model
        self.device = device
        self.tokenizer = tokenizer
        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(self.device)

        # Retrieve Dataloaders
        self.train_dataloader = train_dataloader
        self.valid_dataloader = valid_dataloader
        self.test_dataloader = test_dataloader

        # Quantization
        if use_quantization is True:
            compute_dtype = getattr(torch, "float16")
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=compute_dtype,
                bnb_4bit_use_double_quant=False
            )
            model_kwargs = {
                "use_cache": False,
                "trust_remote_code": True,
                "quantization_config": quant_config,
            }
            self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs).to(self.device)
        else:
            model_kwargs = {
                "use_cache": False,
                "trust_remote_code": True,
            }
            self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs).to(self.device)

        # Peft
        if use_peft is True:
            peft_config = PromptTuningConfig(
                task_type = TaskType.CAUSAL_LM,
                prompt_tuning_init = PromptTuningInit.TEXT,
                num_virtual_tokens = SOFT_PROMPT_LENGTH,
                tokenizer_name_or_path = MODEL_NAME,
                prompt_tuning_init_text="Answer correctly to following question. Try your best to reduce hallucination.",
            )

            self.model = get_peft_model(self.model, peft_config)

        # Model division
        if use_division is True:
            self.model1 = self.FirstHalfLlamaModel(self.model)
            self.model2 = self.SecondHalfLlamaModel(self.model)

        # Trainer configure
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr = LEARNING_RATE)

        lr_scheduler = get_linear_schedule_with_warmup(
            optimizer = self.optimizer,
            num_warmup_steps = 0,
            num_training_steps = (len(self.train_dataloader) * NUM_EPOCHS),
        )

        self.train_losses = []
        self.valid_losses = []

        if 'generated' not in os.listdir('.'):
            os.mkdir('generated')
            os.mkdir('generated/train')
            os.mkdir('generated/valid')

    def train_single_epoch(self, epoch, do_generate = False):
        print(f'[{epoch}/{NUM_EPOCHS}] proceeded.')

        self.model.train()
        train_loss = 0.0
        train_generated = pd.DataFrame(columns = ['prompt', 'answer', 'generated'])

        for index, (prompts, answers, distractors) in enumerate(tqdm(train_dataloader)):
            self.optimizer.zero_grad()

            inputs = self.tokenizer(prompts, padding = 'max_length', max_length = SEQ_LENGTH, truncation = True, return_tensors = 'pt')
            answer_tokens = self.tokenizer([p + a for p, a in zip(prompts, answers)], padding = 'max_length', max_length = SEQ_LENGTH, truncation = True, return_tensors = 'pt')

            inputs['input_ids'] = inputs['input_ids'].to(self.device)
            inputs['attention_mask'] = inputs['attention_mask'].to(self.device)

            outputs = self.model(**inputs, labels = answer_tokens['input_ids'])

            if do_generate:
                generated = self.model.generate(**inputs)

                for p, a, g in zip(prompts, answers, generated):
                    train_generated.loc[len(train_generated)] = {'prompt': p, 'answer': a, 'generated': g}

            loss = outputs.loss
            loss.backward()
            self.optimizer.step()

            train_loss += loss.item()

        self.model.eval()
        valid_loss = 0.0
        valid_generated = pd.DataFrame(columns = ['prompt', 'answer', 'generated'])

        with torch.no_grad():
            for index, (prompts, answers, distractors) in enumerate(tqdm(valid_dataloader)):
                inputs = self.tokenizer(prompts, padding = 'max_length', max_length = SEQ_LENGTH, truncation = True, return_tensors = 'pt')
                answer_tokens = self.tokenizer([p + a for p, a in zip(prompts, answers)], padding = 'max_length', max_length = SEQ_LENGTH, truncation = True, return_tensors = 'pt')

                inputs['input_ids'] = inputs['input_ids'].to(self.device)
                inputs['attention_mask'] = inputs['attention_mask'].to(self.device)

                outputs = self.model(**inputs, labels = answer_tokens['input_ids'])

                if do_generate:
                    generated = self.model.generate(**inputs)

                    for p, a, g in zip(prompts, answers, generated):
                        valid_generated.loc[len(train_generated)] = {'prompt': p, 'answer': a, 'generated': g}

                loss = outputs.loss

                valid_loss += loss.item()

        train_loss = train_loss / len(train_dataloader)
        valid_loss = valid_loss / len(valid_dataloader)

        self.train_losses.append(train_loss)
        self.valid_losses.append(valid_loss)

        if do_generate:
            train_generated.to_csv(f'./generated/train/epoch{epoch}.csv')
            valid_generated.to_csv(f'./generated/valid/epoch{epoch}.csv')

        print(f'train loss: {train_loss}, valid loss: {valid_loss}')

        self.save_losses()

    def save_losses(self):
        with open('train_losses.pkl', 'wb') as f:
            pickle.dump(self.train_losses, f)

        with open('train_losses.pkl', 'wb') as f:
            pickle.dump(self.valid_losses, f)

trainer = Llama3Trainer(
    use_peft = False,
    use_division = False,
    use_quantization = True,
    device = torch.device('cpu'),
    train_dataloader = train_dataloader,
    valid_dataloader = valid_dataloader,
    test_dataloader = test_dataloader,
    tokenizer = tokenizer,
)

for i in range(NUM_EPOCHS):
    trainer.train_single_epoch(0, do_generate = False)
    break

